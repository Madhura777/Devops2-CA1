**DevOps - CA 1 Kaggle Classification Challenge
LLM Classification Fine-tuning**

By:
Madhura Aher (23070122507)
Mayank Verma (23070122508)

**Problem Statement:**
The goal of this project is to build a text classification model to categorize large language model (LLM) prompts into predefined classes. This competition is part of the LLM Classification Fine-tuning Kaggle challenge, where participants use given training data to predict categories for unseen test data.

**Files Overview:**
train.csv: Training dataset containing text samples with their corresponding labels
test.csv: Unlabeled dataset for which predictions are required
sample_submission.csv: Template showing the expected submission format
submission.csv: Final predictions generated by our model for submission
llm_classification.ipynb: Jupyter Notebook containing data preprocessing, feature extraction, model training, and prediction pipeline

**Features:**
The dataset contains:
text: The input prompt or sentence to be classified
label: The target class for classification (in training data only)

**Approach:**

**Data Preprocessing:**

Removed unnecessary characters and cleaned text data

Lowercased all text for uniformity

Applied tokenization where required

**Feature Extraction:**

Used TF-IDF Vectorization to convert text into numerical features

**Model:**

Used LightGBM Classifier for a fast and efficient baseline

Tuned hyperparameters to improve accuracy

**Validation:**

Split training data into 80% training and 20% validation

Measured performance using accuracy score

**Results:**
Validation Accuracy: ~0.65 (baseline)
Kaggle Public Leaderboard: ~0.64
